{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95f46952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from typing import Optional, Union\n",
    "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete\n",
    "from gym import logger, spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2619635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumTon(a, d, n):\n",
    "    return ((n/2)*(2*a+((n-1)*d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a68fff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 50,\n",
    "    }\n",
    "\n",
    "class cartpole(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.n = 2\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        \n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        \n",
    "        self.force_mag = 10.0\n",
    "        \n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "\n",
    "        self.kinematics_integrator = \"euler\"\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        \n",
    "        self.x_threshold = 2.4\n",
    "        \n",
    "        high = np.array(\n",
    "            [\n",
    "                np.arange(4.8, sumTon(4.8, 5.8, self.n), 5.8),\n",
    "                np.full(self.n, np.finfo(np.float32).max),\n",
    "                np.full(self.n, self.theta_threshold_radians * 2),\n",
    "                np.full(self.n, np.finfo(np.float32).max),\n",
    "            ],\n",
    "            dtype=object,\n",
    "        )\n",
    "\n",
    "        low = np.array(\n",
    "            [\n",
    "                np.arange(-4.8, sumTon(-4.8, -5.8, self.n), -5.8),\n",
    "                np.full(self.n, np.finfo(np.float32).max),\n",
    "                np.full(self.n, self.theta_threshold_radians * 2),\n",
    "                np.full(self.n, np.finfo(np.float32).max),\n",
    "            ],\n",
    "            dtype=object,\n",
    "        )\n",
    "        \n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        \n",
    "        \n",
    "        self.action_space = MultiDiscrete(np.full((self.n), 2))\n",
    "        \n",
    "        self.observation_space = spaces.Box(low, high, shape=(4, self.n))\n",
    "\n",
    "        self.render_mode = \"human\"\n",
    "\n",
    "        self.screen_width = 6000 * self.n\n",
    "        self.screen_height = 400 * self.n\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "        self.state = np.random.uniform(low=-0.05, high=0.05, size=(4, self.n))\n",
    "        for i in range(0, self.n):\n",
    "            self.state[0][i] = self.state[0][i] + 5.8\n",
    "\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "        assert self.state is not None, \"Call reset before using step method.\"\n",
    "\n",
    "        x = self.state[0]\n",
    "        x_dot = self.state[1]\n",
    "        theta = self.state[2]\n",
    "        theta_dot = self.state[3]\n",
    "\n",
    "        force = []\n",
    "        for i in range(0, self.n):\n",
    "            force.append(self.force_mag if action[i] == 1 else -self.force_mag)\n",
    "#             force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = np.cos(theta)\n",
    "        sintheta = np.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = np.divide(\n",
    "            np.add(force, (self.polemass_length * np.multiply(np.square(theta_dot), sintheta))),\n",
    "            self.total_mass\n",
    "        )\n",
    "        thetaacc = np.divide(\n",
    "                np.subtract((self.gravity * sintheta),\n",
    "                 np.multiply(costheta, temp)\n",
    "                ),\n",
    "            (\n",
    "            self.length * (4.0 / 3.0 - (self.masspole * (np.square(costheta) / self.total_mass)\n",
    "                                       )\n",
    "                          )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        xacc = np.subtract(temp, (self.polemass_length * np.multiply(thetaacc, costheta))) / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == \"euler\":\n",
    "            x = np.add(x, (self.tau * x_dot))\n",
    "            x_dot = np.add(x_dot, (self.tau * xacc))\n",
    "            theta = np.add(theta, (self.tau * theta_dot))\n",
    "            theta_dot = np.add(theta_dot, (self.tau * thetaacc))\n",
    "\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        reward = []\n",
    "        for i in range(0, self.n):\n",
    "            terminated = bool (x[i] < -self.x_threshold or x[i] > self.x_threshold or theta[i] < -self.theta_threshold_radians or theta[i] > self.theta_threshold_radians)\n",
    "            if not terminated:\n",
    "                reward.append(1.0)\n",
    "            elif self.steps_beyond_terminated is None:\n",
    "                # Pole just fell!\n",
    "                self.steps_beyond_terminated = 0\n",
    "                reward.append(1.0)\n",
    "            else:\n",
    "                if self.steps_beyond_terminated == 0:\n",
    "                    logger.warn(\n",
    "                        \"You are calling 'step()' even though this \"\n",
    "                        \"environment has already returned terminated = True. You \"\n",
    "                        \"should always call 'reset()' once you receive 'terminated = \"\n",
    "                        \"True' -- any further steps are undefined behavior.\"\n",
    "                    )\n",
    "                self.steps_beyond_terminated += 1\n",
    "                reward.append(0.0)\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(self.state).flatten(), reward, terminated, False, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.random.uniform(low=-0.05, high=0.05, size=(4, self.n))\n",
    "        return np.array(self.state).flatten()\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aafd156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = cartpole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecc4981",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "for episodes in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = env.action_space.sample()\n",
    "        print(action)\n",
    "        print(env.step(action))\n",
    "#         obs, reward, done, info = env.step(action)\n",
    "#         score += reward\n",
    "    print(\"Episode:{}, Score:{}\".format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea40062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
